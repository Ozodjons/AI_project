{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow==1.15.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EdPA93odooz0",
        "outputId": "16ed92c6-e8dd-4143-c1ba-614ead942812"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.15.5\n",
            "  Downloading tensorflow-1.15.5-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5 MB 1.3 kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.2.0)\n",
            "Collecting h5py<=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 60.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.8.1)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 51.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.2.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 62.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.37.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (3.3.0)\n",
            "Collecting numpy<1.19.0,>=1.16.0\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (3.17.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.48.1)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (3.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (3.8.1)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=a6da491a68fb5ebdbf2e137cd3eeec2e611321af1123b873ef2f379421be3df1\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: numpy, h5py, tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.18.5 which is incompatible.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n",
            "plotnine 0.8.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.5 which is incompatible.\n",
            "jaxlib 0.3.15+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n",
            "jax 0.3.17 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "cmdstanpy 1.0.7 requires numpy>=1.21, but you have numpy 1.18.5 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 h5py-2.10.0 keras-applications-1.0.8 numpy-1.18.5 tensorboard-1.15.0 tensorflow-1.15.5 tensorflow-estimator-1.15.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YTuehW29pOiR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_parameter = 0.01\n",
        "epochs = 300"
      ],
      "metadata": {
        "id": "MuEF7GVLpmU4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_points=50\n",
        "x_train=np.linspace(0,30,sample_points)\n",
        "y_train=6*x_train+7*np.random.randn(sample_points)"
      ],
      "metadata": {
        "id": "JpQIUJmUpy1R"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(x_train, y_train, 'o')\n",
        "plt.plot(x_train, 6*x_train)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "uQq-MiqLqIgq",
        "outputId": "a399d872-cc8b-41cf-a88c-df5455a0af33"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = tf.placeholder(tf.float32)\n",
        "X = tf.placeholder(tf.float32)\n",
        "\n",
        "W=tf.Variable(np.random.randn(), name='weights')\n",
        "B=tf.Variable(np.random.randn(), name='bias')"
      ],
      "metadata": {
        "id": "Y2Fi9e20qf4S"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction=W*X+B\n",
        "cost_iteration=tf.reduce_sum((prediction-Y)**2)/(2*sample_points)\n",
        "optimizer=tf.train.GradientDescentOptimizer(learning_parameter).minimize(cost_iteration)\n",
        "init=tf.global_variables_initializer()"
      ],
      "metadata": {
        "id": "XKpf7L5YrWx9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98fcf3b9-cd32-4c55-ab27-ae270a4e80f3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  for epoch in range(epochs):\n",
        "    for x, y in zip(x_train, y_train):\n",
        "      if not epoch%40:\n",
        "        W1=sess.run(W)\n",
        "        B1=sess.run(B)\n",
        "        cost_iter=sess.run(cost_iteration, feed_dict={X:x, Y:y})\n",
        "        print('Epochs %f Cost %f Weight %f Bias %f' %(epoch, cost_iter, W1, B1))\n",
        "  Weight=sess.run(W)\n",
        "  Bias=sess.run(B)\n",
        "\n",
        "  plt.plot(x_train, y_train, 'o')\n",
        "  plt.plot(x_train, Weight*x_train+Bias)\n",
        "  plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0WvJoMp1scmQ",
        "outputId": "207e0ed7-3adc-443e-edc4-ebc3fb4d1ffa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs 0.000000 Cost 3.453001 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 0.861901 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 2.034942 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 0.228862 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 2.636491 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 11.650286 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 4.696303 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 11.920534 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 5.772235 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 24.245354 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 24.263178 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 30.348312 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 23.246790 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 23.582016 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 54.589954 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 58.796246 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 67.071724 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 67.673882 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 99.261314 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 72.425529 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 72.192116 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 89.544907 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 87.680450 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 88.747612 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 139.143005 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 103.597672 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 109.853516 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 155.139328 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 153.065598 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 176.796463 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 177.322540 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 152.437653 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 188.326797 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 204.192581 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 252.368622 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 238.941025 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 249.039597 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 253.877029 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 294.167664 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 310.457245 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 285.959534 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 329.302765 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 284.119202 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 312.197968 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 351.285431 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 363.945038 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 385.050842 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 372.181702 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 417.365356 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 431.056824 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 3.453001 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.861901 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 2.034942 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.228862 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 2.636491 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 11.650286 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 4.696303 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 11.920534 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 5.772235 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 24.245354 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 24.263178 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 30.348312 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 23.246790 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 23.582016 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 54.589954 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 58.796246 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 67.071724 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 67.673882 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 99.261314 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 72.425529 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 72.192116 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 89.544907 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 87.680450 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 88.747612 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 139.143005 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 103.597672 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 109.853516 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 155.139328 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 153.065598 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 176.796463 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 177.322540 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 152.437653 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 188.326797 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 204.192581 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 252.368622 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 238.941025 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 249.039597 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 253.877029 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 294.167664 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 310.457245 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 285.959534 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 329.302765 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 284.119202 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 312.197968 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 351.285431 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 363.945038 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 385.050842 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 372.181702 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 417.365356 Weight -0.961007 Bias -0.652841\n",
            "Epochs 40.000000 Cost 431.056824 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 3.453001 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.861901 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 2.034942 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.228862 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 2.636491 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 11.650286 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 4.696303 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 11.920534 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 5.772235 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 24.245354 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 24.263178 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 30.348312 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 23.246790 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 23.582016 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 54.589954 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 58.796246 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 67.071724 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 67.673882 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 99.261314 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 72.425529 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 72.192116 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 89.544907 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 87.680450 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 88.747612 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 139.143005 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 103.597672 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 109.853516 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 155.139328 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 153.065598 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 176.796463 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 177.322540 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 152.437653 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 188.326797 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 204.192581 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 252.368622 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 238.941025 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 249.039597 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 253.877029 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 294.167664 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 310.457245 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 285.959534 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 329.302765 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 284.119202 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 312.197968 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 351.285431 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 363.945038 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 385.050842 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 372.181702 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 417.365356 Weight -0.961007 Bias -0.652841\n",
            "Epochs 80.000000 Cost 431.056824 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 3.453001 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.861901 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 2.034942 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.228862 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 2.636491 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 11.650286 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 4.696303 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 11.920534 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 5.772235 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 24.245354 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 24.263178 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 30.348312 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 23.246790 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 23.582016 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 54.589954 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 58.796246 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 67.071724 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 67.673882 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 99.261314 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 72.425529 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 72.192116 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 89.544907 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 87.680450 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 88.747612 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 139.143005 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 103.597672 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 109.853516 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 155.139328 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 153.065598 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 176.796463 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 177.322540 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 152.437653 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 188.326797 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 204.192581 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 252.368622 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 238.941025 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 249.039597 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 253.877029 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 294.167664 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 310.457245 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 285.959534 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 329.302765 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 284.119202 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 312.197968 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 351.285431 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 363.945038 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 385.050842 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 372.181702 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 417.365356 Weight -0.961007 Bias -0.652841\n",
            "Epochs 120.000000 Cost 431.056824 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 3.453001 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.861901 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 2.034942 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.228862 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 2.636491 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 11.650286 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 4.696303 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 11.920534 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 5.772235 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 24.245354 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 24.263178 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 30.348312 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 23.246790 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 23.582016 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 54.589954 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 58.796246 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 67.071724 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 67.673882 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 99.261314 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 72.425529 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 72.192116 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 89.544907 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 87.680450 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 88.747612 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 139.143005 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 103.597672 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 109.853516 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 155.139328 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 153.065598 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 176.796463 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 177.322540 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 152.437653 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 188.326797 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 204.192581 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 252.368622 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 238.941025 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 249.039597 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 253.877029 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 294.167664 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 310.457245 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 285.959534 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 329.302765 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 284.119202 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 312.197968 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 351.285431 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 363.945038 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 385.050842 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 372.181702 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 417.365356 Weight -0.961007 Bias -0.652841\n",
            "Epochs 160.000000 Cost 431.056824 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 3.453001 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.861901 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 2.034942 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.228862 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 2.636491 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 11.650286 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 4.696303 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 11.920534 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 5.772235 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 24.245354 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 24.263178 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 30.348312 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 23.246790 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 23.582016 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 54.589954 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 58.796246 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 67.071724 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 67.673882 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 99.261314 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 72.425529 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 72.192116 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 89.544907 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 87.680450 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 88.747612 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 139.143005 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 103.597672 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 109.853516 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 155.139328 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 153.065598 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 176.796463 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 177.322540 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 152.437653 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 188.326797 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 204.192581 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 252.368622 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 238.941025 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 249.039597 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 253.877029 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 294.167664 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 310.457245 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 285.959534 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 329.302765 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 284.119202 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 312.197968 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 351.285431 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 363.945038 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 385.050842 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 372.181702 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 417.365356 Weight -0.961007 Bias -0.652841\n",
            "Epochs 200.000000 Cost 431.056824 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 3.453001 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.861901 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 2.034942 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.228862 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 2.636491 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 11.650286 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 4.696303 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 11.920534 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 5.772235 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 24.245354 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 24.263178 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 30.348312 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 23.246790 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 23.582016 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 54.589954 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 58.796246 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 67.071724 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 67.673882 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 99.261314 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 72.425529 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 72.192116 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 89.544907 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 87.680450 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 88.747612 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 139.143005 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 103.597672 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 109.853516 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 155.139328 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 153.065598 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 176.796463 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 177.322540 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 152.437653 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 188.326797 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 204.192581 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 252.368622 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 238.941025 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 249.039597 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 253.877029 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 294.167664 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 310.457245 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 285.959534 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 329.302765 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 284.119202 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 312.197968 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 351.285431 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 363.945038 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 385.050842 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 372.181702 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 417.365356 Weight -0.961007 Bias -0.652841\n",
            "Epochs 240.000000 Cost 431.056824 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 3.453001 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.861901 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 2.034942 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.228862 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 2.636491 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 11.650286 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 4.696303 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 11.920534 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 5.772235 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 24.245354 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 24.263178 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 30.348312 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 23.246790 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 23.582016 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 54.589954 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 58.796246 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 67.071724 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 67.673882 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 99.261314 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 72.425529 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 72.192116 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 89.544907 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 87.680450 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 88.747612 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 139.143005 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 103.597672 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 109.853516 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 155.139328 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 153.065598 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 176.796463 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 177.322540 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 152.437653 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 188.326797 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 204.192581 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 252.368622 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 238.941025 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 249.039597 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 253.877029 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 294.167664 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 310.457245 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 285.959534 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 329.302765 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 284.119202 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 312.197968 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 351.285431 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 363.945038 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 385.050842 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 372.181702 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 417.365356 Weight -0.961007 Bias -0.652841\n",
            "Epochs 280.000000 Cost 431.056824 Weight -0.961007 Bias -0.652841\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"Model\") as scope:\n",
        "  prediction=W*X+Bias\n",
        "  weight_histogram=tf.summary.histogram(\"Weights\", W)\n",
        "  bias_histogram=tf.summary.histogram(\"Bias\", B)\n",
        "\n",
        "with tf.name_scope(\"Cost_function\") as scope:\n",
        "  cost_iteration=tf.reduce_sum((prediction-Y)**2)/(2*sample_points)\n",
        "\n",
        "cost_summary=tf.summary.scalar(\"Cost\", cost_iteration)"
      ],
      "metadata": {
        "id": "pgRNeuxmt6oF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"Training\") as scope:\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_parameter).minimize(cost_iteration)\n",
        "  \n",
        "  init=tf.global_variables_initializer()\n",
        "\n",
        "  merged_summary=tf.summary.merge_all()"
      ],
      "metadata": {
        "id": "25uzTYi2vBiv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  writer=tf.summary.FileWriter('./log', sess.graph)\n",
        "  for epoch in range(epochs):\n",
        "    for x, y in zip(x_train, y_train):\n",
        "      sess.run(optimizer, feed_dict={X:x, Y:y})\n",
        "\n",
        "      summary_epochs=sess.run(merged_summary, feed_dict={X:x, Y:y})\n",
        "      writer.add_summary(summary_epochs, epoch)\n",
        "      if not epoch%40:\n",
        "        W1=sess.run(W)\n",
        "        B1=sess.run(B)\n",
        "        cost_iter=sess.run(cost_iteration, feed_dict={X:x, Y:y})\n",
        "        print('Epochs %f Cost %f Weight %f Bias %f' %(epoch, cost_iter, W1, B1))\n",
        "  Weight=sess.run(W)\n",
        "  Bias=sess.run(B)\n",
        "\n",
        "  plt.plot(x_train, y_train, 'o')\n",
        "  plt.plot(x_train, Weight*x_train+Bias)\n",
        "  plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b9ed92fa-7700-472b-d3d9-df0a454b2914",
        "id": "NRL5cNHbwl4b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs 0.000000 Cost 3.453001 Weight -0.961007 Bias -0.652841\n",
            "Epochs 0.000000 Cost 0.861772 Weight -0.959870 Bias -0.652841\n",
            "Epochs 0.000000 Cost 2.033325 Weight -0.956377 Bias -0.652841\n",
            "Epochs 0.000000 Cost 0.227741 Weight -0.954623 Bias -0.652841\n",
            "Epochs 0.000000 Cost 2.625107 Weight -0.946678 Bias -0.652841\n",
            "Epochs 0.000000 Cost 11.576843 Weight -0.925807 Bias -0.652841\n",
            "Epochs 0.000000 Cost 4.615412 Weight -0.909981 Bias -0.652841\n",
            "Epochs 0.000000 Cost 11.683691 Weight -0.880574 Bias -0.652841\n",
            "Epochs 0.000000 Cost 5.531026 Weight -0.857425 Bias -0.652841\n",
            "Epochs 0.000000 Cost 23.399733 Weight -0.803790 Bias -0.652841\n",
            "Epochs 0.000000 Cost 22.975771 Weight -0.744653 Bias -0.652841\n",
            "Epochs 0.000000 Cost 28.244667 Weight -0.672414 Bias -0.652841\n",
            "Epochs 0.000000 Cost 20.790890 Weight -0.604683 Bias -0.652841\n",
            "Epochs 0.000000 Cost 20.381557 Weight -0.531896 Bias -0.652841\n",
            "Epochs 0.000000 Cost 47.852230 Weight -0.411541 Bias -0.652841\n",
            "Epochs 0.000000 Cost 49.595818 Weight -0.279971 Bias -0.652841\n",
            "Epochs 0.000000 Cost 54.438156 Weight -0.132590 Bias -0.652841\n",
            "Epochs 0.000000 Cost 51.906708 Weight 0.020706 Bias -0.652841\n",
            "Epochs 0.000000 Cost 75.088959 Weight 0.216453 Bias -0.652841\n",
            "Epochs 0.000000 Cost 48.265942 Weight 0.382581 Bias -0.652841\n",
            "Epochs 0.000000 Cost 44.168358 Weight 0.550370 Bias -0.652841\n",
            "Epochs 0.000000 Cost 52.867592 Weight 0.743732 Bias -0.652841\n",
            "Epochs 0.000000 Cost 46.391861 Weight 0.934125 Bias -0.652841\n",
            "Epochs 0.000000 Cost 42.044449 Weight 1.124281 Bias -0.652841\n",
            "Epochs 0.000000 Cost 69.801605 Weight 1.380889 Bias -0.652841\n",
            "Epochs 0.000000 Cost 39.498779 Weight 1.582739 Bias -0.652841\n",
            "Epochs 0.000000 Cost 37.282188 Weight 1.787508 Bias -0.652841\n",
            "Epochs 0.000000 Cost 55.944695 Weight 2.049090 Bias -0.652841\n",
            "Epochs 0.000000 Cost 46.075912 Weight 2.296352 Bias -0.652841\n",
            "Epochs 0.000000 Cost 49.552052 Weight 2.563140 Bias -0.652841\n",
            "Epochs 0.000000 Cost 40.724743 Weight 2.814528 Bias -0.652841\n",
            "Epochs 0.000000 Cost 23.112118 Weight 3.011185 Bias -0.652841\n",
            "Epochs 0.000000 Cost 30.084023 Weight 3.243973 Bias -0.652841\n",
            "Epochs 0.000000 Cost 28.310978 Weight 3.478091 Bias -0.652841\n",
            "Epochs 0.000000 Cost 36.840210 Weight 3.754763 Bias -0.652841\n",
            "Epochs 0.000000 Cost 23.628742 Weight 3.984155 Bias -0.652841\n",
            "Epochs 0.000000 Cost 19.423120 Weight 4.199337 Bias -0.652841\n",
            "Epochs 0.000000 Cost 14.502481 Weight 4.391606 Bias -0.652841\n",
            "Epochs 0.000000 Cost 17.553453 Weight 4.610220 Bias -0.652841\n",
            "Epochs 0.000000 Cost 14.629143 Weight 4.816382 Bias -0.652841\n",
            "Epochs 0.000000 Cost 5.906703 Weight 4.951646 Bias -0.652841\n",
            "Epochs 0.000000 Cost 8.342075 Weight 5.117557 Bias -0.652841\n",
            "Epochs 0.000000 Cost 1.130418 Weight 5.180570 Bias -0.652841\n",
            "Epochs 0.000000 Cost 1.670533 Weight 5.259575 Bias -0.652841\n",
            "Epochs 0.000000 Cost 2.879825 Weight 5.366529 Bias -0.652841\n",
            "Epochs 0.000000 Cost 1.945215 Weight 5.457135 Bias -0.652841\n",
            "Epochs 0.000000 Cost 1.694430 Weight 5.544279 Bias -0.652841\n",
            "Epochs 0.000000 Cost 0.228363 Weight 5.577240 Bias -0.652841\n",
            "Epochs 0.000000 Cost 1.010434 Weight 5.648657 Bias -0.652841\n",
            "Epochs 0.000000 Cost 0.585212 Weight 5.704632 Bias -0.652841\n",
            "Epochs 40.000000 Cost 3.453001 Weight 5.983792 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.253167 Weight 5.984408 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.331640 Weight 5.985819 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.635232 Weight 5.982889 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.005887 Weight 5.982513 Bias -0.652841\n",
            "Epochs 40.000000 Cost 1.651922 Weight 5.990396 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.148562 Weight 5.987557 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.223647 Weight 5.991625 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.996028 Weight 5.981802 Bias -0.652841\n",
            "Epochs 40.000000 Cost 1.191717 Weight 5.993906 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.439107 Weight 6.002081 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.659457 Weight 6.013119 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.089456 Weight 6.008677 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.465685 Weight 5.997674 Bias -0.652841\n",
            "Epochs 40.000000 Cost 1.968394 Weight 6.022085 Bias -0.652841\n",
            "Epochs 40.000000 Cost 1.521919 Weight 6.045133 Bias -0.652841\n",
            "Epochs 40.000000 Cost 1.692903 Weight 6.071123 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.787837 Weight 6.090008 Bias -0.652841\n",
            "Epochs 40.000000 Cost 4.576326 Weight 6.138332 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.060067 Weight 6.144193 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.039024 Weight 6.139206 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.104284 Weight 6.147794 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.041481 Weight 6.142101 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.312112 Weight 6.125717 Bias -0.652841\n",
            "Epochs 40.000000 Cost 1.750419 Weight 6.166353 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.485366 Weight 6.143977 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.619158 Weight 6.117589 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.508263 Weight 6.142521 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.033511 Weight 6.149189 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.396737 Weight 6.173061 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.039401 Weight 6.180881 Bias -0.652841\n",
            "Epochs 40.000000 Cost 1.257501 Weight 6.135009 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.027365 Weight 6.127988 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.000922 Weight 6.126652 Bias -0.652841\n",
            "Epochs 40.000000 Cost 1.069343 Weight 6.173789 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.023520 Weight 6.181026 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.001263 Weight 6.182761 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.050048 Weight 6.171466 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.247067 Weight 6.197402 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.218226 Weight 6.222582 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.360366 Weight 6.189172 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.030034 Weight 6.199127 Bias -0.652841\n",
            "Epochs 40.000000 Cost 1.822966 Weight 6.119108 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.698649 Weight 6.068015 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.027141 Weight 6.057632 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.048529 Weight 6.043321 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.007621 Weight 6.037476 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.498856 Weight 5.988760 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.000022 Weight 5.989096 Bias -0.652841\n",
            "Epochs 40.000000 Cost 0.005255 Weight 5.983792 Bias -0.652841\n",
            "Epochs 80.000000 Cost 3.453001 Weight 5.983792 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.253167 Weight 5.984408 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.331640 Weight 5.985819 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.635232 Weight 5.982889 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.005887 Weight 5.982513 Bias -0.652841\n",
            "Epochs 80.000000 Cost 1.651922 Weight 5.990396 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.148562 Weight 5.987557 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.223647 Weight 5.991625 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.996028 Weight 5.981802 Bias -0.652841\n",
            "Epochs 80.000000 Cost 1.191717 Weight 5.993906 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.439107 Weight 6.002081 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.659457 Weight 6.013119 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.089456 Weight 6.008677 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.465685 Weight 5.997674 Bias -0.652841\n",
            "Epochs 80.000000 Cost 1.968394 Weight 6.022085 Bias -0.652841\n",
            "Epochs 80.000000 Cost 1.521919 Weight 6.045133 Bias -0.652841\n",
            "Epochs 80.000000 Cost 1.692903 Weight 6.071123 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.787837 Weight 6.090008 Bias -0.652841\n",
            "Epochs 80.000000 Cost 4.576326 Weight 6.138332 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.060067 Weight 6.144193 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.039024 Weight 6.139206 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.104284 Weight 6.147794 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.041481 Weight 6.142101 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.312112 Weight 6.125717 Bias -0.652841\n",
            "Epochs 80.000000 Cost 1.750419 Weight 6.166353 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.485366 Weight 6.143977 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.619158 Weight 6.117589 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.508263 Weight 6.142521 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.033511 Weight 6.149189 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.396737 Weight 6.173061 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.039401 Weight 6.180881 Bias -0.652841\n",
            "Epochs 80.000000 Cost 1.257501 Weight 6.135009 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.027365 Weight 6.127988 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.000922 Weight 6.126652 Bias -0.652841\n",
            "Epochs 80.000000 Cost 1.069343 Weight 6.173789 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.023520 Weight 6.181026 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.001263 Weight 6.182761 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.050048 Weight 6.171466 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.247067 Weight 6.197402 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.218226 Weight 6.222582 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.360366 Weight 6.189172 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.030034 Weight 6.199127 Bias -0.652841\n",
            "Epochs 80.000000 Cost 1.822966 Weight 6.119108 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.698649 Weight 6.068015 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.027141 Weight 6.057632 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.048529 Weight 6.043321 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.007621 Weight 6.037476 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.498856 Weight 5.988760 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.000022 Weight 5.989096 Bias -0.652841\n",
            "Epochs 80.000000 Cost 0.005255 Weight 5.983792 Bias -0.652841\n",
            "Epochs 120.000000 Cost 3.453001 Weight 5.983792 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.253167 Weight 5.984408 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.331640 Weight 5.985819 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.635232 Weight 5.982889 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.005887 Weight 5.982513 Bias -0.652841\n",
            "Epochs 120.000000 Cost 1.651922 Weight 5.990396 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.148562 Weight 5.987557 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.223647 Weight 5.991625 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.996028 Weight 5.981802 Bias -0.652841\n",
            "Epochs 120.000000 Cost 1.191717 Weight 5.993906 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.439107 Weight 6.002081 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.659457 Weight 6.013119 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.089456 Weight 6.008677 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.465685 Weight 5.997674 Bias -0.652841\n",
            "Epochs 120.000000 Cost 1.968394 Weight 6.022085 Bias -0.652841\n",
            "Epochs 120.000000 Cost 1.521919 Weight 6.045133 Bias -0.652841\n",
            "Epochs 120.000000 Cost 1.692903 Weight 6.071123 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.787837 Weight 6.090008 Bias -0.652841\n",
            "Epochs 120.000000 Cost 4.576326 Weight 6.138332 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.060067 Weight 6.144193 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.039024 Weight 6.139206 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.104284 Weight 6.147794 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.041481 Weight 6.142101 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.312112 Weight 6.125717 Bias -0.652841\n",
            "Epochs 120.000000 Cost 1.750419 Weight 6.166353 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.485366 Weight 6.143977 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.619158 Weight 6.117589 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.508263 Weight 6.142521 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.033511 Weight 6.149189 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.396737 Weight 6.173061 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.039401 Weight 6.180881 Bias -0.652841\n",
            "Epochs 120.000000 Cost 1.257501 Weight 6.135009 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.027365 Weight 6.127988 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.000922 Weight 6.126652 Bias -0.652841\n",
            "Epochs 120.000000 Cost 1.069343 Weight 6.173789 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.023520 Weight 6.181026 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.001263 Weight 6.182761 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.050048 Weight 6.171466 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.247067 Weight 6.197402 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.218226 Weight 6.222582 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.360366 Weight 6.189172 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.030034 Weight 6.199127 Bias -0.652841\n",
            "Epochs 120.000000 Cost 1.822966 Weight 6.119108 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.698649 Weight 6.068015 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.027141 Weight 6.057632 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.048529 Weight 6.043321 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.007621 Weight 6.037476 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.498856 Weight 5.988760 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.000022 Weight 5.989096 Bias -0.652841\n",
            "Epochs 120.000000 Cost 0.005255 Weight 5.983792 Bias -0.652841\n",
            "Epochs 160.000000 Cost 3.453001 Weight 5.983792 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.253167 Weight 5.984408 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.331640 Weight 5.985819 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.635232 Weight 5.982889 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.005887 Weight 5.982513 Bias -0.652841\n",
            "Epochs 160.000000 Cost 1.651922 Weight 5.990396 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.148562 Weight 5.987557 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.223647 Weight 5.991625 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.996028 Weight 5.981802 Bias -0.652841\n",
            "Epochs 160.000000 Cost 1.191717 Weight 5.993906 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.439107 Weight 6.002081 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.659457 Weight 6.013119 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.089456 Weight 6.008677 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.465685 Weight 5.997674 Bias -0.652841\n",
            "Epochs 160.000000 Cost 1.968394 Weight 6.022085 Bias -0.652841\n",
            "Epochs 160.000000 Cost 1.521919 Weight 6.045133 Bias -0.652841\n",
            "Epochs 160.000000 Cost 1.692903 Weight 6.071123 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.787837 Weight 6.090008 Bias -0.652841\n",
            "Epochs 160.000000 Cost 4.576326 Weight 6.138332 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.060067 Weight 6.144193 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.039024 Weight 6.139206 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.104284 Weight 6.147794 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.041481 Weight 6.142101 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.312112 Weight 6.125717 Bias -0.652841\n",
            "Epochs 160.000000 Cost 1.750419 Weight 6.166353 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.485366 Weight 6.143977 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.619158 Weight 6.117589 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.508263 Weight 6.142521 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.033511 Weight 6.149189 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.396737 Weight 6.173061 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.039401 Weight 6.180881 Bias -0.652841\n",
            "Epochs 160.000000 Cost 1.257501 Weight 6.135009 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.027365 Weight 6.127988 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.000922 Weight 6.126652 Bias -0.652841\n",
            "Epochs 160.000000 Cost 1.069343 Weight 6.173789 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.023520 Weight 6.181026 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.001263 Weight 6.182761 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.050048 Weight 6.171466 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.247067 Weight 6.197402 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.218226 Weight 6.222582 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.360366 Weight 6.189172 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.030034 Weight 6.199127 Bias -0.652841\n",
            "Epochs 160.000000 Cost 1.822966 Weight 6.119108 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.698649 Weight 6.068015 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.027141 Weight 6.057632 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.048529 Weight 6.043321 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.007621 Weight 6.037476 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.498856 Weight 5.988760 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.000022 Weight 5.989096 Bias -0.652841\n",
            "Epochs 160.000000 Cost 0.005255 Weight 5.983792 Bias -0.652841\n",
            "Epochs 200.000000 Cost 3.453001 Weight 5.983792 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.253167 Weight 5.984408 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.331640 Weight 5.985819 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.635232 Weight 5.982889 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.005887 Weight 5.982513 Bias -0.652841\n",
            "Epochs 200.000000 Cost 1.651922 Weight 5.990396 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.148562 Weight 5.987557 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.223647 Weight 5.991625 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.996028 Weight 5.981802 Bias -0.652841\n",
            "Epochs 200.000000 Cost 1.191717 Weight 5.993906 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.439107 Weight 6.002081 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.659457 Weight 6.013119 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.089456 Weight 6.008677 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.465685 Weight 5.997674 Bias -0.652841\n",
            "Epochs 200.000000 Cost 1.968394 Weight 6.022085 Bias -0.652841\n",
            "Epochs 200.000000 Cost 1.521919 Weight 6.045133 Bias -0.652841\n",
            "Epochs 200.000000 Cost 1.692903 Weight 6.071123 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.787837 Weight 6.090008 Bias -0.652841\n",
            "Epochs 200.000000 Cost 4.576326 Weight 6.138332 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.060067 Weight 6.144193 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.039024 Weight 6.139206 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.104284 Weight 6.147794 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.041481 Weight 6.142101 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.312112 Weight 6.125717 Bias -0.652841\n",
            "Epochs 200.000000 Cost 1.750419 Weight 6.166353 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.485366 Weight 6.143977 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.619158 Weight 6.117589 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.508263 Weight 6.142521 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.033511 Weight 6.149189 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.396737 Weight 6.173061 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.039401 Weight 6.180881 Bias -0.652841\n",
            "Epochs 200.000000 Cost 1.257501 Weight 6.135009 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.027365 Weight 6.127988 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.000922 Weight 6.126652 Bias -0.652841\n",
            "Epochs 200.000000 Cost 1.069343 Weight 6.173789 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.023520 Weight 6.181026 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.001263 Weight 6.182761 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.050048 Weight 6.171466 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.247067 Weight 6.197402 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.218226 Weight 6.222582 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.360366 Weight 6.189172 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.030034 Weight 6.199127 Bias -0.652841\n",
            "Epochs 200.000000 Cost 1.822966 Weight 6.119108 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.698649 Weight 6.068015 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.027141 Weight 6.057632 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.048529 Weight 6.043321 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.007621 Weight 6.037476 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.498856 Weight 5.988760 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.000022 Weight 5.989096 Bias -0.652841\n",
            "Epochs 200.000000 Cost 0.005255 Weight 5.983792 Bias -0.652841\n",
            "Epochs 240.000000 Cost 3.453001 Weight 5.983792 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.253167 Weight 5.984408 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.331640 Weight 5.985819 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.635232 Weight 5.982889 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.005887 Weight 5.982513 Bias -0.652841\n",
            "Epochs 240.000000 Cost 1.651922 Weight 5.990396 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.148562 Weight 5.987557 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.223647 Weight 5.991625 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.996028 Weight 5.981802 Bias -0.652841\n",
            "Epochs 240.000000 Cost 1.191717 Weight 5.993906 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.439107 Weight 6.002081 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.659457 Weight 6.013119 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.089456 Weight 6.008677 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.465685 Weight 5.997674 Bias -0.652841\n",
            "Epochs 240.000000 Cost 1.968394 Weight 6.022085 Bias -0.652841\n",
            "Epochs 240.000000 Cost 1.521919 Weight 6.045133 Bias -0.652841\n",
            "Epochs 240.000000 Cost 1.692903 Weight 6.071123 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.787837 Weight 6.090008 Bias -0.652841\n",
            "Epochs 240.000000 Cost 4.576326 Weight 6.138332 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.060067 Weight 6.144193 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.039024 Weight 6.139206 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.104284 Weight 6.147794 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.041481 Weight 6.142101 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.312112 Weight 6.125717 Bias -0.652841\n",
            "Epochs 240.000000 Cost 1.750419 Weight 6.166353 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.485366 Weight 6.143977 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.619158 Weight 6.117589 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.508263 Weight 6.142521 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.033511 Weight 6.149189 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.396737 Weight 6.173061 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.039401 Weight 6.180881 Bias -0.652841\n",
            "Epochs 240.000000 Cost 1.257501 Weight 6.135009 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.027365 Weight 6.127988 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.000922 Weight 6.126652 Bias -0.652841\n",
            "Epochs 240.000000 Cost 1.069343 Weight 6.173789 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.023520 Weight 6.181026 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.001263 Weight 6.182761 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.050048 Weight 6.171466 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.247067 Weight 6.197402 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.218226 Weight 6.222582 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.360366 Weight 6.189172 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.030034 Weight 6.199127 Bias -0.652841\n",
            "Epochs 240.000000 Cost 1.822966 Weight 6.119108 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.698649 Weight 6.068015 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.027141 Weight 6.057632 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.048529 Weight 6.043321 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.007621 Weight 6.037476 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.498856 Weight 5.988760 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.000022 Weight 5.989096 Bias -0.652841\n",
            "Epochs 240.000000 Cost 0.005255 Weight 5.983792 Bias -0.652841\n",
            "Epochs 280.000000 Cost 3.453001 Weight 5.983792 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.253167 Weight 5.984408 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.331640 Weight 5.985819 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.635232 Weight 5.982889 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.005887 Weight 5.982513 Bias -0.652841\n",
            "Epochs 280.000000 Cost 1.651922 Weight 5.990396 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.148562 Weight 5.987557 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.223647 Weight 5.991625 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.996028 Weight 5.981802 Bias -0.652841\n",
            "Epochs 280.000000 Cost 1.191717 Weight 5.993906 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.439107 Weight 6.002081 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.659457 Weight 6.013119 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.089456 Weight 6.008677 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.465685 Weight 5.997674 Bias -0.652841\n",
            "Epochs 280.000000 Cost 1.968394 Weight 6.022085 Bias -0.652841\n",
            "Epochs 280.000000 Cost 1.521919 Weight 6.045133 Bias -0.652841\n",
            "Epochs 280.000000 Cost 1.692903 Weight 6.071123 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.787837 Weight 6.090008 Bias -0.652841\n",
            "Epochs 280.000000 Cost 4.576326 Weight 6.138332 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.060067 Weight 6.144193 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.039024 Weight 6.139206 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.104284 Weight 6.147794 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.041481 Weight 6.142101 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.312112 Weight 6.125717 Bias -0.652841\n",
            "Epochs 280.000000 Cost 1.750419 Weight 6.166353 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.485366 Weight 6.143977 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.619158 Weight 6.117589 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.508263 Weight 6.142521 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.033511 Weight 6.149189 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.396737 Weight 6.173061 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.039401 Weight 6.180881 Bias -0.652841\n",
            "Epochs 280.000000 Cost 1.257501 Weight 6.135009 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.027365 Weight 6.127988 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.000922 Weight 6.126652 Bias -0.652841\n",
            "Epochs 280.000000 Cost 1.069343 Weight 6.173789 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.023520 Weight 6.181026 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.001263 Weight 6.182761 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.050048 Weight 6.171466 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.247067 Weight 6.197402 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.218226 Weight 6.222582 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.360366 Weight 6.189172 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.030034 Weight 6.199127 Bias -0.652841\n",
            "Epochs 280.000000 Cost 1.822966 Weight 6.119108 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.698649 Weight 6.068015 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.027141 Weight 6.057632 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.048529 Weight 6.043321 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.007621 Weight 6.037476 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.498856 Weight 5.988760 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.000022 Weight 5.989096 Bias -0.652841\n",
            "Epochs 280.000000 Cost 0.005255 Weight 5.983792 Bias -0.652841\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow==1.15.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dvmp-nRBoHuv",
        "outputId": "0ba10057-3116-496d-f2e6-fe47009d8e76"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow==1.15.5 in /usr/local/lib/python3.7/dist-packages (1.15.5)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.15.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.14.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.2.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.2.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.18.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (3.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.48.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.8.1)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.0.8)\n",
            "Requirement already satisfied: h5py<=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (2.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.37.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (3.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzdbDFqCpgFN",
        "outputId": "20e237d9-5675-4689-c03c-289dc968dcef"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-04 11:06:45--  https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.202.168.65, 18.205.222.128, 54.161.241.46, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.202.168.65|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8454063 (8.1M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-v3-stable-linux-amd64.zip’\n",
            "\n",
            "\r          ngrok-v3-   0%[                    ]       0  --.-KB/s               \rngrok-v3-stable-lin 100%[===================>]   8.06M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-10-04 11:06:45 (70.9 MB/s) - ‘ngrok-v3-stable-linux-amd64.zip’ saved [8454063/8454063]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip ngrok-v3-stable-linux-amd64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vpt_eUK1pkWZ",
        "outputId": "6237591b-e539-49bd-8c7a-785eb04d2f33"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ngrok-v3-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LOG_DIR = './log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "metadata": {
        "id": "1YO86vXDpsAA"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "metadata": {
        "id": "hhMzrMzkr1bP"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_kxdJYwr2vS",
        "outputId": "78ddb4dd-a5dc-49c9-df53-32fecba2fe95"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://5c6a-34-86-24-192.ngrok.io\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow==1.15.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMRkbxQ9tdrk",
        "outputId": "c8261e44-43ac-4213-ce60-116fc45cebff"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow==1.15.5 in /usr/local/lib/python3.7/dist-packages (1.15.5)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.14.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.15.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.2.2)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.15.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.0.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.48.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (3.17.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.1.2)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.18.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.37.1)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (3.3.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.8.1)\n",
            "Requirement already satisfied: h5py<=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (2.10.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (3.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (3.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "33piId4Er6L9"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist_data = input_data.read_data_sets(\"./data\", one_hot = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfVeQSWctHae",
        "outputId": "5680534e-96e8-4fea-aad8-bc5282160567"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/train-images-idx3-ubyte.gz\n",
            "Extracting ./data/train-labels-idx1-ubyte.gz\n",
            "Extracting ./data/t10k-images-idx3-ubyte.gz\n",
            "Extracting ./data/t10k-labels-idx1-ubyte.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_training = mnist_data.train.num_examples\n",
        "num_testing = mnist_data.test.num_examples\n",
        "num_validation = mnist_data.validation.num_examples\n",
        "print(\"MNIST Datasize: Training samples: {0}, Testing samples: {1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RP_h0GmVtLbS",
        "outputId": "766e9cc2-873a-4f72-e256-f3b5fe9f5cd6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST Datasize: Training samples: {0}, Testing samples: {1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_input = 784\n",
        "n_hidden_1 = 512\n",
        "n_hidden_2 = 256\n",
        "n_hidden_3 = 128\n",
        "n_output = 10"
      ],
      "metadata": {
        "id": "ny_iqOWQtppl"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-4\n",
        "epochs = 3000\n",
        "batch_size = 128\n",
        "keep_prob = tf.placeholder(tf.float32)"
      ],
      "metadata": {
        "id": "2n4VfDR7tvyR"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = tf.placeholder(tf.float32, [None, n_input])\n",
        "Y = tf.placeholder(tf.float32, [None, n_output])"
      ],
      "metadata": {
        "id": "NfpSkwNBtwNR"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_weight = {\"W1\": tf.Variable(tf.truncated_normal([n_input, n_hidden_1], stddev = 0.1)),\n",
        "             \"W2\": tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2], stddev = 0.1)),\n",
        "             \"W3\": tf.Variable(tf.truncated_normal([n_hidden_2, n_hidden_3], stddev = 0.1)),\n",
        "             \"Wout\":tf.Variable(tf.truncated_normal([n_hidden_3, n_output]))\n",
        "}\n",
        "\n",
        "nn_bias = { \"B1\": tf.Variable(tf.truncated_normal([n_hidden_1])),\n",
        "            \"B2\": tf.Variable(tf.truncated_normal([n_hidden_2])),\n",
        "            \"B3\": tf.Variable(tf.truncated_normal([n_hidden_3])),\n",
        "            \"B4\": tf.Variable(tf.truncated_normal([n_output])),  \n",
        "           }"
      ],
      "metadata": {
        "id": "AmUJiVtVtx3N"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_layer_1 = tf.add(tf.matmul(X, nn_weight[\"W1\"]),nn_bias[\"B1\"])\n",
        "nn_layer_2 = tf.add(tf.matmul(nn_layer_1, nn_weight[\"W2\"]),nn_bias[\"B2\"])\n",
        "nn_layer_3 = tf.add(tf.matmul(nn_layer_2, nn_weight[\"W3\"]),nn_bias[\"B3\"])\n",
        "layer_drop = tf.nn.dropout(nn_layer_3, keep_prob)\n",
        "output_layer = tf.add(tf.matmul(layer_drop, nn_weight[\"Wout\"]), nn_bias[\"B4\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5p9dht-Btzms",
        "outputId": "26236060-5faa-458b-839c-7e9819853ae7"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-43-7479a4162d01>:4: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "computed_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = output_layer, labels = Y))\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(computed_loss)\n",
        "\n",
        "prediction_out = tf.equal(tf.argmax(output_layer,1), tf.argmax(Y,1))\n",
        "\n",
        "nn_accuracy = tf.reduce_mean(tf.cast(prediction_out, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "saver = tf.train.Saver()"
      ],
      "metadata": {
        "id": "_SQeonJpt9-n"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  for i in range(epochs):\n",
        "\n",
        "    mini_batch_x, mini_batch_y = mnist_data.train.next_batch(batch_size)\n",
        "    mini_batch_val_x, mini_batch_val_y = mnist_data.validation.next_batch(batch_size)\n",
        "\n",
        "    sess.run(optimizer, feed_dict = {X : mini_batch_x, Y : mini_batch_y, keep_prob:1})\n",
        "    \n",
        "    if i%100 == 0:\n",
        "      mini_batch_loss, mini_batch_accuracy = sess.run([computed_loss, nn_accuracy], feed_dict = {X: mini_batch_x, Y: mini_batch_y, keep_prob:1})\n",
        "\n",
        "      mini_batch_val_loss, mini_batch_val_accuracy = sess.run([computed_loss, nn_accuracy], feed_dict = {X: mini_batch_x, Y: mini_batch_y, keep_prob:1})\n",
        "\n",
        "      print(\"Iterations : {0} , Train_loss = {1}, Train_Accuracy {2}, Val_loss {3}, Val_accuracy {4}\".format(i, mini_batch_loss, mini_batch_accuracy, mini_batch_val_loss, mini_batch_val_accuracy))\n",
        "\n",
        "  print(\"Optimization Finished\")\n",
        "  test_accuracy = sess.run(nn_accuracy, feed_dict = {X:mnist_data.test.images, Y:mnist_data.test.labels, keep_prob:1.0})\n",
        "  print(\"Testing accuracy is {0}\".format(test_accuracy))\n",
        "\n",
        "  saver_path = saver.save(sess, \"./model/my_model.ckpt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PspSO69juBHX",
        "outputId": "f06d2bbc-ca2b-4a4c-b748-613f1fb3881e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iterations : 0 , Train_loss = 62.75988006591797, Train_Accuracy 0.09375, Val_loss 62.75988006591797, Val_accuracy 0.09375\n",
            "Iterations : 100 , Train_loss = 7.630047798156738, Train_Accuracy 0.3984375, Val_loss 7.630047798156738, Val_accuracy 0.3984375\n",
            "Iterations : 200 , Train_loss = 5.156005859375, Train_Accuracy 0.5390625, Val_loss 5.156005859375, Val_accuracy 0.5390625\n",
            "Iterations : 300 , Train_loss = 4.031378269195557, Train_Accuracy 0.640625, Val_loss 4.031378269195557, Val_accuracy 0.640625\n",
            "Iterations : 400 , Train_loss = 2.320164203643799, Train_Accuracy 0.7265625, Val_loss 2.320164203643799, Val_accuracy 0.7265625\n",
            "Iterations : 500 , Train_loss = 3.4700002670288086, Train_Accuracy 0.703125, Val_loss 3.4700002670288086, Val_accuracy 0.703125\n",
            "Iterations : 600 , Train_loss = 2.6428496837615967, Train_Accuracy 0.7578125, Val_loss 2.6428496837615967, Val_accuracy 0.7578125\n",
            "Iterations : 700 , Train_loss = 1.9757579565048218, Train_Accuracy 0.7734375, Val_loss 1.9757579565048218, Val_accuracy 0.7734375\n",
            "Iterations : 800 , Train_loss = 1.7412383556365967, Train_Accuracy 0.828125, Val_loss 1.7412383556365967, Val_accuracy 0.828125\n",
            "Iterations : 900 , Train_loss = 1.7462730407714844, Train_Accuracy 0.78125, Val_loss 1.7462730407714844, Val_accuracy 0.78125\n",
            "Iterations : 1000 , Train_loss = 1.035443902015686, Train_Accuracy 0.8671875, Val_loss 1.035443902015686, Val_accuracy 0.8671875\n",
            "Iterations : 1100 , Train_loss = 2.3231797218322754, Train_Accuracy 0.7890625, Val_loss 2.3231797218322754, Val_accuracy 0.7890625\n",
            "Iterations : 1200 , Train_loss = 1.9099071025848389, Train_Accuracy 0.78125, Val_loss 1.9099071025848389, Val_accuracy 0.78125\n",
            "Iterations : 1300 , Train_loss = 1.9094319343566895, Train_Accuracy 0.796875, Val_loss 1.9094319343566895, Val_accuracy 0.796875\n",
            "Iterations : 1400 , Train_loss = 1.3296639919281006, Train_Accuracy 0.7890625, Val_loss 1.3296639919281006, Val_accuracy 0.7890625\n",
            "Iterations : 1500 , Train_loss = 1.2893362045288086, Train_Accuracy 0.8359375, Val_loss 1.2893362045288086, Val_accuracy 0.8359375\n",
            "Iterations : 1600 , Train_loss = 2.1789724826812744, Train_Accuracy 0.7734375, Val_loss 2.1789724826812744, Val_accuracy 0.7734375\n",
            "Iterations : 1700 , Train_loss = 1.5353842973709106, Train_Accuracy 0.8125, Val_loss 1.5353842973709106, Val_accuracy 0.8125\n",
            "Iterations : 1800 , Train_loss = 1.7664786577224731, Train_Accuracy 0.8125, Val_loss 1.7664786577224731, Val_accuracy 0.8125\n",
            "Iterations : 1900 , Train_loss = 2.027729034423828, Train_Accuracy 0.765625, Val_loss 2.027729034423828, Val_accuracy 0.765625\n",
            "Iterations : 2000 , Train_loss = 1.0812937021255493, Train_Accuracy 0.8671875, Val_loss 1.0812937021255493, Val_accuracy 0.8671875\n",
            "Iterations : 2100 , Train_loss = 1.1615461111068726, Train_Accuracy 0.8125, Val_loss 1.1615461111068726, Val_accuracy 0.8125\n",
            "Iterations : 2200 , Train_loss = 1.7286365032196045, Train_Accuracy 0.859375, Val_loss 1.7286365032196045, Val_accuracy 0.859375\n",
            "Iterations : 2300 , Train_loss = 1.654350996017456, Train_Accuracy 0.8203125, Val_loss 1.654350996017456, Val_accuracy 0.8203125\n",
            "Iterations : 2400 , Train_loss = 2.1978437900543213, Train_Accuracy 0.796875, Val_loss 2.1978437900543213, Val_accuracy 0.796875\n",
            "Iterations : 2500 , Train_loss = 1.3056917190551758, Train_Accuracy 0.859375, Val_loss 1.3056917190551758, Val_accuracy 0.859375\n",
            "Iterations : 2600 , Train_loss = 0.8272609710693359, Train_Accuracy 0.828125, Val_loss 0.8272609710693359, Val_accuracy 0.828125\n",
            "Iterations : 2700 , Train_loss = 1.1851369142532349, Train_Accuracy 0.859375, Val_loss 1.1851369142532349, Val_accuracy 0.859375\n",
            "Iterations : 2800 , Train_loss = 1.535198450088501, Train_Accuracy 0.8203125, Val_loss 1.535198450088501, Val_accuracy 0.8203125\n",
            "Iterations : 2900 , Train_loss = 0.9287463426589966, Train_Accuracy 0.8515625, Val_loss 0.9287463426589966, Val_accuracy 0.8515625\n",
            "Optimization Finished\n",
            "Testing accuracy is 0.8485000133514404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread(\"7.jpg\")\n",
        "gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "rescaled_image = cv2.resize(gray_image, (28,28))\n",
        "plt.imshow(rescaled_image, cmap = 'gray')\n",
        "plt.show()\n",
        "rescaled_image.shape\n",
        "#test_image = rescaled_image.flatten()\n",
        "\n",
        "dum = rescaled_image.reshape(1,-1)/255\n",
        "dum.shape\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, \"./model/my_model.ckpt\")\n",
        "  Z = output_layer.eval(feed_dict = {X:dum, keep_prob:1.0})\n",
        "  y_pred = np.argmax(Z, axis = 1)\n",
        "  print(\"Prediction for test image is {0}\".format(y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "RITrqelAuHgB",
        "outputId": "80966a7b-ab43-4d7a-d3b9-18a72e6f73c7"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for test image is [0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZlpTFN_6uSxF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}